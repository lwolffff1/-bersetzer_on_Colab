import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
import json
import time

#driver 
driver = None
def init_driver():
    global driver  
    if driver is None:
        options = webdriver.ChromeOptions()
        options.add_argument('--headless')
        options.add_argument('--no-sandbox')
        driver= webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
    return driver
def close_driver():
    global driver
    if driver is not None:
        driver.quit()
        driver = None

# Kiá»ƒm tra xem trang cÃ³ pháº£i tÄ©nh khÃ´ng
def is_static_page(url):
    try:
        response = requests.get(url, timeout=5)
        if response.status_code == 200:
            soup = BeautifulSoup(response.content, "html.parser")
            static_text = " ".join([p.text.strip() for p in soup.find_all("p") if p.text.strip()])
            return static_text
    except requests.exceptions.RequestException:
        return None
    return None

# Kiá»ƒm tra trang JavaScript
def scrape_with_selenium(url):
    global driver
    try:
        if driver is None:
            driver = init_driver()
        else:
            driver.get(url)  # Táº£i láº¡i trang náº¿u driver Ä‘Ã£ Ä‘Æ°á»£c khá»Ÿi táº¡o
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "p"))
            )

        js_text = " ".join([p.text.strip() for p in driver.find_elements(By.TAG_NAME, 'p') if p.text.strip()])
        
        return js_text
    except Exception as e:
        print(f"âŒ Error loading page with Selenium: {e}")
        return None
   

# LÆ°u vÄƒn báº£n vÃ o file JSON
def save_text_to_json(article_obj, filename="output.json"):
    try: 
        # Äá»c dá»¯ liá»‡u cÅ©
        with open(filename, "r", encoding="utf-8") as f:
            data = json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
            data = []
    # Kiá»ƒm tra xem URL Ä‘Ã£ tá»“n táº¡i chÆ°a
    existing_urls = {item["url"] for item in data}
    if article_obj["url"] in existing_urls:
        print(f"â­ ÄÃ£ tá»“n táº¡i: {article_obj['url']}")
        return
    # ThÃªm bÃ i má»›i
    data.append(article_obj)
    
    with open(filename,"w", encoding = "utf-8") as file:
        json.dump(data, file, ensure_ascii=False, indent=2)
    print (f"ğŸ“Œ Saved to {filename}")  


#HÃ m phÃ¢n loáº¡i xem cÃ¡i nÃ o cáº§n dÃ¹ng BeutifulSoup cÃ o, cÃ¡i nÃ o cáº§n dÃ¹ng Selenium
def classify_scraping_method(url):
    static_text = is_static_page(url)
    if static_text:
        print("\nğŸ“Œ This website is STATIC. Using BeautifulSoup to scrape content.")
        return static_text
    else:
        print("\nâš  This website may use JavaScript. Checking with Selenium...")
        dynamic_text = scrape_with_selenium(url)
        if dynamic_text:
            print(f"\nğŸš€ Website {url} requires JavaScript. Content length: {len(dynamic_text)} characters")
            return dynamic_text
        else:
            print("\nâŒ I can't load the website.")
            return None
# HÃ m lÃ m sáº¡ch vÄƒn báº£n
def clean_repetitive_text(text, min_repeat=3, min_paragraph_length=50):
    if not text:
        return text
    # BÆ°á»›c 1: LÃ m sÃ¡ch tá»« láº·p liÃªn tiáº¿p
    words = text.split(" ")
    cleaned_words = []
    last_word = None
    repeat_count = 1
        
    for word in words:
        if word == last_word:
            repeat_count += 1
        else:
            repeat_count = 1
            last_word = word
            
        if repeat_count <= min_repeat:
            cleaned_words.append(word)
        
    intermediate_text = ' '.join(cleaned_words) 

    # BÆ°á»›c 2: Xá»­ lÃ½ Ä‘oáº¡n vÄƒn láº·p láº¡i
    paragraphs = [p.strip() for p in intermediate_text.split('\n') if p.strip()]
    unique_paragraphs = []
    seen_paragraphs = set() 
    for para in paragraphs:
        # Chá»‰ xem xÃ©t cÃ¡c Ä‘oáº¡n Ä‘á»§ dÃ i
        if len(para.split()) >= min_paragraph_length:
            # Chuáº©n hÃ³a Ä‘oáº¡n vÄƒn Ä‘á»ƒ so sÃ¡nh (bá» qua khoáº£ng tráº¯ng thá»«a, chuyá»ƒn thÃ nh chá»¯ thÆ°á»ng)
            normalized = ' '.join(para.lower().split())
            if normalized not in seen_paragraphs:
                seen_paragraphs.add(normalized)
                unique_paragraphs.append(para)
        else:
            unique_paragraphs.append(para)  # Giá»¯ láº¡i cÃ¡c Ä‘oáº¡n ngáº¯n
    # BÆ°á»›c 3: Loáº¡i bá» cÃ¡c máº«u quáº£ng cÃ¡o/thÃ´ng bÃ¡o láº·p láº¡i
    common_patterns = [
        "cookie policy", "privacy policy", "terms of service",
        "please enable javascript", "we use cookies",
        "Ä‘ang táº£i...", "loading...", "click Ä‘á»ƒ xem thÃªm"
    ]
    final_paragraphs = []
    for para in unique_paragraphs:
        if not any(pattern.lower() in para.lower() for pattern in common_patterns):
            final_paragraphs.append(para)
    
    return '\n'.join(final_paragraphs)
       
# HÃ m chÃ­nh
if __name__ == "__main__":
    try: 
        with open("article_links.json", "r", encoding="utf-8") as f:
            url = json.load(f)

        for item in url:
            input_url = item["content"]
            text = classify_scraping_method(input_url)
            if text:
                cleaned_text = clean_repetitive_text(text)
                
        
                article_obj = {
                        "url": input_url,
                        "content": cleaned_text
                }
        
                save_text_to_json(article_obj)  # âœ… Ghi tá»«ng bÃ i sau khi scrape
            else:
                print(f"âŒ Failed to scrape the article: {input_url}")

        print("ğŸ“Œ Finished scraping all articles.")
    finally:
        close_driver()  # ÄÃ³ng driver khi hoÃ n thÃ nh


